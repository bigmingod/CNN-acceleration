深度神经网络DNN 尤其是深度卷及神经网络CNN 取得巨大成就在视觉领域 通过学习大量数据 然而部署庞大的模型需要庞大的计算和存储。为了减少计算开销 实施了许多研究来压缩DNN的规模 包括稀疏正则化 连接剪枝 低秩近似 前两个方法通常产生DNN非结构随机链接 因此不规律的内存访问 不利地影响硬件的加速 图片1描述了alexnet每一层的实际加速 这是非结构l1-norm(相加)。和原始模型对比，稀疏模型的精确度损失控制在2%。因为数据局部性和分散的权值分布不联系，得到的局速比或者很有限 或者是负的 即使实际的分散率很高 大于95%。我们定义零散率为数字0的比例。最近提出的低秩近似方法中 DNN被训练然后 每个被训练的权值张量被小因数的乘积分解近似。最终 精细的兼职用来存储模型的精确度。低秩近似能够得到实际的加速因为它协调模型参数在稠密的矩阵中，防止了非结构的稀疏正则化的局部性问题。然而，低秩近似只能得到每层的紧凑结构 每层的结构是固定的 在精细剪枝的过程中 以至于分解的开销大的迭代和精细剪枝需要寻找最佳的权值近似 为了加速的表现 和精确度。
以下事实：（1）filter和channel中有冗余，（2）filter的形状通常固定为立方形但是使随意的形状能够潜在的消除有这种固定带来的不必要的计算（3）网络的深度对分类来说是很重要的 但是更深的网络不能保证更低的错误率 因为渐变和退化（？）.这些事实激励我们，我们提出了结构化分散学习SSL方法来直接学习一个深度CNN的压缩结构 在训练中用组稀疏约束LASSO正则化。SSL是一个通用的正则化来适应性调整DNN的多重结构 包括每一层filter，channel filter形状的结构，还有层之外深度的结构。SSL把结构正则化（在DNN上为了分类精确度）和局部性最优化（在内存访问为了计算的效率）相结合，提供了不仅是好的正则化模型以及提高的精确度 还有巨大的加速计算率。


2 相关工作
连接剪枝和权值稀疏 一些人减少参数的数量用连接剪枝。因为大部分的减少都是在全连接层，作者得到的加速率是全连接层的。然后，卷积层的实际加速率没有观察到 因为图1中的问题。卷积是计算的瓶颈，许多新的DNN用更少的全连接层，对卷积的压缩和加速就更加重要.又一些人在Alexnet卷积层达到90%以上的分散率精确度在2%内，绕过图1的问题通过硬编码分散权值到程序中，达到了CPU层4.59。在这次工作中，我们也关注卷积层。相比上面的技术，我们的SSL方法能够协调分散的权值，通过相邻的存储空间，以相同的精确度达到更高的加速率。注意硬件和程序优化能够进一步提高系统表现在SSL级别，但是不包含在这次工作中。

低秩近似 一些人预测95%的DNN参数 在利用filter和channel的冗余。由此激励，又一些人达到了4.5倍CPU加速对语义识别，还有人在前两层到达2倍。这些工作用到低秩近似1%准确率下降。别的人改进和拓展了低秩近似到庞大的DNN。然而，被低秩近似压缩的网络结构是固定的，分解的迭代、训练剪枝、和交叉验证仍然需要最佳的结构来保证精确度和加速的二者权衡。低秩近似的超参数随着层的深度线性增加，搜索空间也线性增加，甚至对深的DNN是几何增长。对比低秩近似，我们的贡献有：（1）SSL可以动态的优化DNN结构的紧实程度只用一个超参数不用迭代。（2）除了层之间冗余，SSL利用了深层的必要性病减少了他们。（3）被SSL正则化的DNNfilter有低秩近似，所以可以和LRA一起得到更有效的模型压缩。

模型压缩学习 Group Lasso是有效的正则化来学习稀疏结构。一些人用Group LASSO来正则化关系树为了多任务回归问题和减少预测错误。还有一波人利用group LASSO来限制LRA结构的规模。为了让DNN架构适应不同的数据基，另一拨人学习了DNNfilter的近似数。不同于这些先前的技术，我们用group LASSO来正则化多重DNN的架构（filter，channel filter shape和层深）。这是我们的代码。

3 DNN的结构化稀疏学习方法
我们只要关注SSL在卷积层来正则化DNN结构。首先3.1我们提出正则化DNN结构的通用方法。3.2明确方法对滤波器结构、通道、滤波器形状和深度。3.3从计算效率的角度讨论公式的变量。
3.1 提出SSL
假设DNN中卷积层的权值是一个4维张量序列，N、C、M、K是第l个权值张量的维度在滤波器(核)、通道、空间相对高度、宽度。L代表了卷基层的数量。提出的结构化稀疏的DNN通用优化目标可以写成这样。

3.2 结构化稀疏学习为了核 通道和通道形状深度
SSL里面学习的结构由分离的组wg决定。我们研究病公理化面向核、面向形状、面向深度的结构化稀疏在图片2中。为了简化，方程1中的r术语就省略了。
处罚不重要的核和通道。假设W是第nl个核,w是Cl个通道在第l层的所有核中。学习面向核和面向通道结构化稀疏的优化目标可以定义成以下这样。如方程2表明，我们的方法趋向于移除更不重要的核和通道。注意到把L层的一个核归零在假0输出特征图中，也会使了l+1层一个相关的通道没用。因此，我们结合核和通道的结构化稀疏同时学习。
学习任意的核形状。图2所示，W指示了所有相关的权值的向量，权值在空间位置m，k在2D的核在C通道。因此，我们定义W作为形状纤维，与学习任意的核形状相联系，因为一个同质的非立方的核形状可以被学习，通过把一些形状纤维归零。学习核形状的优化目标为这个。
正则化层的深度 我们还探索了深度稀疏来正则化DNN的深度为了改善准确度减少计算开销。相关的优化目标是这样的。不同于其他讨论的稀疏技术，把一层中所有的和归零会切断DNN的信息传播以至于输出神经不能表现任何的分类。受到高速网络的结构和深度剩余网络的启发，我们提出利用一些层之间的捷径来解决。如图2所示，即使当SSL移除了整个不重要的层，特征图还是会通过捷径向前传递。

3.3 结构化稀疏学习为了计算高效结构
从3.2提出的框架可以学习一个紧实的DNN为了减少计算开销。不仅如此，一些公式中的变量可以直接学习能被有效计算的结构。
2D-核稀疏为了卷积 3D卷积本质上是2D卷积的组合。为了展现有效卷积，我们探索了一个细粒的变量为了核稀疏，也就是说，2D核稀疏，空间上强迫每个2D核Group LASSO。这保留的卷积和移除的2D核的百分比成正比。核稀疏的细粒版本可以更有效的减少与卷积相关的计算。因为组的大小更小了因此权值更新率更高了。帮助Group LASSO更快的得到了一个高的0组的比例在庞大的DNN中。
结合核稀疏和形状稀疏for GEMM。 DNN中卷积计算通常转换为GEMM的形式 通过降低权值张量和特征张量的维度到矩阵。例如，caffe中，3D核被重新塑形到权值矩阵的一行，每一列都是与形状稀疏相关的权值的和。结合核稀疏和形状稀疏可以直接减少GEMM中权值矩阵的维度通过移除0的行列。我们用行列稀疏作为核和形状稀疏术语的替换。


5 结论
此次工作中，我们提出了结构化稀疏学习的方法来正则化核、通道、核形状和深度在DNN中。我们的方法可以使DNN动态的学习更紧实的结构而精确度不下降。这个DNN紧实结构达到了很好的加速率在CPU和GPU。不仅如此，SSL的变化可以显示作为结构正则化来改善最先进的DNN的分类准确率

 




























